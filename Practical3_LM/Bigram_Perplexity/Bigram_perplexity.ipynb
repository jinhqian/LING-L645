{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNMnGTFl/Y4HR90udaUZMfr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jinhqian/LING-L645/blob/main/Practical3_LM/Bigram_Perplexity/Bigram_perplexity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4FBcXVrlrcgl",
        "outputId": "ebb989f6-8f1b-4718-b80c-85ce365f699e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import sys, math, re, pickle\n",
        "import nltk\n",
        "from collections import defaultdict, Counter\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_train = [\"are you still here?\",\n",
        "\"where are you?\",\n",
        "\"are you tired?\",\n",
        "\"i am tired.\",\n",
        "\"are you in england?\",\n",
        "\"were you in mexico?\"]\n",
        "\n",
        "corpus_test = [\"where are you?\",\n",
        "\"were you in england?\",\n",
        "\"are you in mexico?\",\n",
        "\"i am in mexico.\",\n",
        "\"are you still in mexico?\"]"
      ],
      "metadata": {
        "id": "1nMIfsPVre9f"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_bigrams, count_unigrams = defaultdict(Counter), Counter()\n",
        "model = defaultdict(lambda : defaultdict(float)) \n",
        "\n",
        "def Ngrams(corpus):\n",
        "    list_unigrams = []\n",
        "    list_bigrams = []\n",
        "    for line in corpus:\n",
        "        tokens = word_tokenize(line)\n",
        "        tokens = ['<BOS>'] + tokens\n",
        "     \n",
        "        for i in range(len(tokens)):\n",
        "            count_unigrams[tokens[i]] += 1\n",
        "            list_unigrams.append(tokens[i])\n",
        "        \n",
        "        for i in range(len(tokens) - 1):\n",
        "            count_bigrams[tokens[i]][tokens[i+1]] += 1\n",
        "            list_bigrams.append((tokens[i], tokens[i + 1]))\n",
        "    return list_unigrams, list_bigrams \n",
        "\n",
        "list_unigrams,list_bigrams  = Ngrams(corpus_train)\n",
        "print(f'Unigrams:\\n{list_unigrams}', f'\\ntotal unigrams: {len(list_unigrams)}')\n",
        "print(f'\\nBigrams:\\n{list_bigrams}' f'\\ntotal bigrams: {len(list_bigrams)}')\n",
        "print(f'\\nUnigram counts:\\n{count_unigrams}')\n",
        "print(f'\\nBigram counts:\\n{count_bigrams}')\n",
        "\n",
        "# unigram probability\n",
        "def unigrams_prob(unigrams, total_unigrams):\n",
        "    unigrams_prob = {}\n",
        "    for unigram in unigrams:\n",
        "        unigrams_prob[unigram] = unigrams[unigram]/total_unigrams\n",
        "    return unigrams_prob\n",
        "\n",
        "# Probability of sentences (unigram)\n",
        "def unigram_sentence_prob(unigram_prob): \n",
        "    unigram_sentence_prob = 1\n",
        "    for p in unigram_prob.values():\n",
        "        unigram_sentence_prob *= p\n",
        "    return unigram_sentence_prob\n",
        "\n",
        "unigram_prob = unigrams_prob(count_unigrams, len(list_unigrams))\n",
        "print(f'\\nUnigram probability:\\n{unigram_prob}')\n",
        "unigram_sentence_prob = unigram_sentence_prob(unigram_prob)\n",
        "unigram_perplexity = unigram_sentence_prob ** (-1/len(list_unigrams)) # Unigram perplexity\n",
        "print(f'\\nUnigram perplexity:\\n{unigram_perplexity}')\n",
        "\n",
        "# bigram probability\n",
        "for i in count_bigrams: \n",
        "    for j in count_bigrams[i]:\n",
        "        model[i][j] = (count_bigrams[i][j] + 1) / (count_unigrams[i] + len(count_unigrams))\n",
        "        # model[i][j] = count_bigrams[i][j] / count_unigrams[i]\n",
        "print(f'\\nBigram probability:\\n{model}')\n",
        "pickle.dump(dict(model), open('model.lm', 'wb'))\n",
        "\n",
        "# Probability of sentences (bigram)\n",
        "print('\\nProbability of sentences (bigram):')\n",
        "for line in corpus_train:\n",
        "    tokens = word_tokenize(line)\n",
        "    tokens = ['<BOS>'] + tokens\n",
        "    # print(tokens)\n",
        "    P = 1\n",
        "    for i in range(len(tokens) - 1):  \n",
        "        P *= model[tokens[i]][tokens[i+1]]\n",
        "    \n",
        "    print('%.6f' % P, '%.6f' % math.log(P), tokens)\n",
        "\n",
        "# bigram perplexity\n",
        "bigram_perplexity = P ** (-1/len(list_unigrams))\n",
        "print(f'\\nBigram perplexity:\\n{bigram_perplexity}')\n",
        "print('\\n##### In train data #####')\n",
        "print(f'Unigram perplexity: {unigram_perplexity}, Bigram perplexity: {bigram_perplexity}')\n",
        "print(\"Bigram has lower perplexity than Unigram\")\n",
        "print('''\n",
        "##### In test data #####\n",
        "# Unigram perplexity: 3.3479024417796, Bigram perplexity: 1.4136346948208365\n",
        "# Bigram has lower perplexity than Unigram ''')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cfkPcPn0IRp",
        "outputId": "a383e35c-317d-4d1c-aed3-eb44a484b35a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unigrams:\n",
            "['<BOS>', 'are', 'you', 'still', 'here', '?', '<BOS>', 'where', 'are', 'you', '?', '<BOS>', 'are', 'you', 'tired', '?', '<BOS>', 'i', 'am', 'tired', '.', '<BOS>', 'are', 'you', 'in', 'england', '?', '<BOS>', 'were', 'you', 'in', 'mexico', '?'] \n",
            "total unigrams: 33\n",
            "\n",
            "Bigrams:\n",
            "[('<BOS>', 'are'), ('are', 'you'), ('you', 'still'), ('still', 'here'), ('here', '?'), ('<BOS>', 'where'), ('where', 'are'), ('are', 'you'), ('you', '?'), ('<BOS>', 'are'), ('are', 'you'), ('you', 'tired'), ('tired', '?'), ('<BOS>', 'i'), ('i', 'am'), ('am', 'tired'), ('tired', '.'), ('<BOS>', 'are'), ('are', 'you'), ('you', 'in'), ('in', 'england'), ('england', '?'), ('<BOS>', 'were'), ('were', 'you'), ('you', 'in'), ('in', 'mexico'), ('mexico', '?')]\n",
            "total bigrams: 27\n",
            "\n",
            "Unigram counts:\n",
            "Counter({'<BOS>': 6, 'you': 5, '?': 5, 'are': 4, 'tired': 2, 'in': 2, 'still': 1, 'here': 1, 'where': 1, 'i': 1, 'am': 1, '.': 1, 'england': 1, 'were': 1, 'mexico': 1})\n",
            "\n",
            "Bigram counts:\n",
            "defaultdict(<class 'collections.Counter'>, {'<BOS>': Counter({'are': 3, 'where': 1, 'i': 1, 'were': 1}), 'are': Counter({'you': 4}), 'you': Counter({'in': 2, 'still': 1, '?': 1, 'tired': 1}), 'still': Counter({'here': 1}), 'here': Counter({'?': 1}), 'where': Counter({'are': 1}), 'tired': Counter({'?': 1, '.': 1}), 'i': Counter({'am': 1}), 'am': Counter({'tired': 1}), 'in': Counter({'england': 1, 'mexico': 1}), 'england': Counter({'?': 1}), 'were': Counter({'you': 1}), 'mexico': Counter({'?': 1})})\n",
            "\n",
            "Unigram probability:\n",
            "{'<BOS>': 0.18181818181818182, 'are': 0.12121212121212122, 'you': 0.15151515151515152, 'still': 0.030303030303030304, 'here': 0.030303030303030304, '?': 0.15151515151515152, 'where': 0.030303030303030304, 'tired': 0.06060606060606061, 'i': 0.030303030303030304, 'am': 0.030303030303030304, '.': 0.030303030303030304, 'in': 0.06060606060606061, 'england': 0.030303030303030304, 'were': 0.030303030303030304, 'mexico': 0.030303030303030304}\n",
            "\n",
            "Unigram perplexity:\n",
            "3.87081990216204\n",
            "\n",
            "Bigram probability:\n",
            "defaultdict(<function <lambda> at 0x7fd996525ee0>, {'<BOS>': defaultdict(<class 'float'>, {'are': 0.19047619047619047, 'where': 0.09523809523809523, 'i': 0.09523809523809523, 'were': 0.09523809523809523}), 'are': defaultdict(<class 'float'>, {'you': 0.2631578947368421}), 'you': defaultdict(<class 'float'>, {'still': 0.1, '?': 0.1, 'tired': 0.1, 'in': 0.15}), 'still': defaultdict(<class 'float'>, {'here': 0.125}), 'here': defaultdict(<class 'float'>, {'?': 0.125}), 'where': defaultdict(<class 'float'>, {'are': 0.125}), 'tired': defaultdict(<class 'float'>, {'?': 0.11764705882352941, '.': 0.11764705882352941}), 'i': defaultdict(<class 'float'>, {'am': 0.125}), 'am': defaultdict(<class 'float'>, {'tired': 0.125}), 'in': defaultdict(<class 'float'>, {'england': 0.11764705882352941, 'mexico': 0.11764705882352941}), 'england': defaultdict(<class 'float'>, {'?': 0.125}), 'were': defaultdict(<class 'float'>, {'you': 0.125}), 'mexico': defaultdict(<class 'float'>, {'?': 0.125})})\n",
            "\n",
            "Probability of sentences (bigram):\n",
            "0.000078 -9.454697 ['<BOS>', 'are', 'you', 'still', 'here', '?']\n",
            "0.000313 -8.068403 ['<BOS>', 'where', 'are', 'you', '?']\n",
            "0.000590 -7.435880 ['<BOS>', 'are', 'you', 'tired', '?']\n",
            "0.000175 -8.650325 ['<BOS>', 'i', 'am', 'tired', '.']\n",
            "0.000111 -9.109857 ['<BOS>', 'are', 'you', 'in', 'england', '?']\n",
            "0.000026 -10.547444 ['<BOS>', 'were', 'you', 'in', 'mexico', '?']\n",
            "\n",
            "Bigram perplexity:\n",
            "1.3766039081518864\n",
            "\n",
            "##### In train data #####\n",
            "Unigram perplexity: 3.87081990216204, Bigram perplexity: 1.3766039081518864\n",
            "Bigram has lower perplexity than Unigram\n",
            "\n",
            "##### In test data #####\n",
            "# Unigram perplexity: 3.3479024417796, Bigram perplexity: 1.4136346948208365\n",
            "# Bigram has lower perplexity than Unigram \n"
          ]
        }
      ]
    }
  ]
}