{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fz1ZvvkHabFK"
      },
      "outputs": [],
      "source": [
        "import sys, re\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "from model import *\n",
        "\n",
        "###############################################################################\n",
        "\n",
        "training_samples = []\n",
        "vocabulary = set(['<UNK>'])\n",
        "line = sys.stdin.readline()\n",
        "while line:\n",
        "    tokens = preprocess(line)\n",
        "    for i in tokens: vocabulary.add(i) \n",
        "    training_samples.append(tokens)\n",
        "    line = sys.stdin.readline()\n",
        "\n",
        "word2idx = {k: v for v, k in enumerate(vocabulary)}\n",
        "idx2word = {v: k for k, v in word2idx.items()}\n",
        "\n",
        "x_train = []\n",
        "y_train = []\n",
        "for tokens in training_samples:\n",
        "    # for i in range(len(tokens) - 1): #!!!#\n",
        "    for i in range(len(tokens) - 2): #!!!#\n",
        "        # x_train.append([word2idx[tokens[i]]]) #!!!#\n",
        "        x_train.append([word2idx[tokens[i]], word2idx[tokens[i+1]]] ) #!!!#\n",
        "        # y_train.append([word2idx[tokens[i+1]]]) #!!!#\n",
        "        y_train.append([word2idx[tokens[i+2]]]) #!!!#\n",
        "\n",
        "x_train = np.array(x_train)\n",
        "y_train = np.array(y_train)\n",
        "\n",
        "###############################################################################\n",
        "\n",
        "BATCH_SIZE = 1\n",
        "NUM_EPOCHS = 10\n",
        "\n",
        "train_set = np.concatenate((x_train, y_train), axis=1)\n",
        "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE)\n",
        "\n",
        "loss_function = nn.NLLLoss()\n",
        "model = TrigramNNmodel(len(vocabulary), EMBEDDING_DIM, CONTEXT_SIZE, HIDDEN_DIM)\n",
        "optimiser = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    for i, data_tensor in enumerate(train_loader):\n",
        "        # context_tensor = data_tensor[:,0:1] #!!!#\n",
        "        context_tensor = data_tensor[:,0:2] #!!!#\n",
        "        # target_tensor = data_tensor[:,1] #!!!#\n",
        "        target_tensor = data_tensor[:,2] #!!!#\n",
        "\n",
        "        model.zero_grad()\n",
        "\n",
        "        log_probs = model(context_tensor)\n",
        "        loss = loss_function(log_probs, target_tensor)\n",
        "\n",
        "        loss.backward()\n",
        "        optimiser.step()    \n",
        "\n",
        "    print('Epoch:', epoch, 'loss:', float(loss))\n",
        "\n",
        "torch.save({'model': model.state_dict(), 'vocab': idx2word}, 'model.lm')\n",
        "\n",
        "print('Model saved.')"
      ]
    }
  ]
}
